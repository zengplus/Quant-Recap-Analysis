# AlphaGPT 实验文档 V0.2

> **版本**: V0.2  
> **日期**: 2026-02-11  
> **核心任务**: 将“单一年份(2025)失效”问题转化为可复现的稳健性评估体系，并对齐可解释投资目标（牛熊规则）

本文档对 AlphaGPT 项目 V0.2 版本进行结构化梳理。相较 V0.1 更强调“搭管线/对齐回测/防未来函数”，V0.2 的主线是：在强化学习高方差与市场结构变化的背景下，重构时间窗切分与验收口径，使实验更可解释、结论更可复现、对 2025 的讨论更符合方法论（压力测试而非训练目标）。

---

## 1. 机器学习全流程 (Pipeline Overview)

V0.2 沿用 V0.1 的三段式流程，但把“评估体系与目标定义”提升为第一等公民：

1. **数据工程 (Data Engineering)**
   - 延续 V0.1：截面口径、滚动窗口、物理切片、避免未来函数。
2. **模型训练 (Model Training)**
   - 延续 V0.1：AlphaGPT 生成公式，使用 REINFORCE 优化。
   - V0.2 重点：训练/验证的“选择逻辑”与“验收口径”重构，减少选择偏置对最终结论的影响。
3. **回测验证 (Backtesting)**
   - 延续 V0.1：JoinQuant-like 为主裁判，Qlib 交叉核验。
   - V0.2 重点：统一参数入口（如 TOPK），并将多时间段回测视为标准输出（而非只看单一年份）。

---

## 2. V0.2 的核心变化（相比 V0.1）

### 2.1 从“2025 必须通过”转为“2025 压力测试”

V0.2 的关键结论是：把 2025 直接作为模型选择目标，会在方法论与工程上同时引入风险：
- **方法论风险**：不断围绕 2025 改 reward/切分/筛选，容易形成测试集驱动的选择偏置。
- **工程风险**：REINFORCE 高方差 + 金融低信噪比背景下，强行对单一年份施加硬约束会放大极端月噪声影响，降低收敛与命中概率。

因此 V0.2 将 2025 明确为“压力测试”，不参与模型选择。

### 2.2 时间窗切分重构：train / valid / holdout + pressure test

V0.2 使用更稳健的时间切分设计来减少近邻年份结构相关性带来的选择偏置：

- **训练段(train)**：2014-2019（远离 2025，减少近邻结构影响）
- **验证段(valid)**：2020-2022（重点覆盖熊市环境，检验核心能力）
- **最终验证段(holdout)**：2023-2024（更近期样本外检验，但不用于最终挑选）
- **压力测试**：2025（完全隔离的最终 out-of-sample）

同时保留了对“原三段（短窗/长窗）”失败现象的对照记录，用于说明为什么需要重构评估体系。

### 2.3 验收口径升级为“牛熊规则”（可解释、可推导、可复现）

定义：
- 基准收益 **B**，策略总收益 **T**，策略超额 **E**，满足恒等式 **T = B + E**

验收规则（V0.2）：
- **牛市/基准为正（B ≥ 0）**：要求 **E > 0**
- **熊市/基准为负（B < 0）**：要求 **E > 0 且 T > 0**，等价于 **E > |B|**

回测汇总字段映射：
- `BenchReturn%` ↔ B
- `TotalReturn%` ↔ T
- `ExcessReturn%` ↔ E

---

## 3. 奖励函数与选择机制（V0.2 的试验与结论）

### 3.1 为什么要动奖励函数

在原三段框架下，常见失败现象包括：
- valid 表现好但 test_2025 超额为负（结构失配）
- 通过某个 seed 但难以多 seed 复现（可行解稀疏 + 高方差）

因此 V0.2 尝试通过 reward 引导模型远离“跑输基准但绝对收益尚可”的解，并提升稳健性。

### 3.2 做过的 reward 方向性试验（配置注入，便于对照/回滚）

V0.2 的 reward/评估相关可调项集中在 `model_core/config.py` 与 `model_core/ashare_backtest.py`：
- 负超额惩罚：`NEG_EXCESS_PENALTY`
- 负绝对收益惩罚：`NEG_PORT_PENALTY`
- 尾部/最差段稳健项：`MIN_EXCESS_WEIGHT` 与 `TAIL_SEG_FRACTION`
- 牛/熊市场权重：`ALPHA_WEIGHT_BULL` / `ALPHA_WEIGHT_BEAR`，并结合 `MARKET_REGIME_THRESHOLD` 识别市场状态

### 3.3 结论：用 gate 替代“硬惩罚锁死”

实验现象表明：当惩罚与尾部约束过强时，ValidBest 容易被极端月份噪声主导，训练会偏向“躲极端坏值”而不是“找 alpha”，并不提升多 seed 通过率。

因此 V0.2 的策略是：
- reward 作为方向引导可保留，但不强行锁死；
- 把牛熊规则更倾向于设计为训练期的 gate/多阶段筛选规则（后续工作方向）。

---

## 4. 回测与复现（V0.2 的可复现性要求）

### 4.1 关键工程点：参数口径统一

为避免“改了参数但回测没生效”的误判，V0.2 强调统一入口读取配置（例如 TOPK 由 `ModelConfig.TOPK` 控制）。

### 4.2 最小复现方式（训练 + 多阶段回测汇总）

以下示例展示如何在一个 seed 下训练，并产出 train/valid/holdout/test_2025 的汇总表：

```python
from run_pipeline import train_ashare, run_multi_backtest

seed = 20240202
output_dir = f"outputs/v0.2_repro_seed_{seed}"

formula, *_ = train_ashare(
    train_start="2014-01-01",
    train_end="2019-12-31",
    valid_start="2020-01-01",
    valid_end="2022-12-31",
    valid_periods=[
        ("2020-01-01", "2020-12-31"),
        ("2021-01-01", "2021-12-31"),
        ("2022-01-01", "2022-12-31"),
    ],
    valid_agg="min",
    instruments="csi300",
    batch_size=128,
    train_steps=300,
    max_formula_len=12,
    output_dir=output_dir,
    seed=seed,
    token_mode="postfix",
)

summary_df = run_multi_backtest(
    formula,
    periods=[
        ("train", "2014-01-01", "2019-12-31"),
        ("valid", "2020-01-01", "2022-12-31"),
        ("holdout", "2023-01-01", "2024-12-31"),
        ("test_2025", "2025-01-01", "2025-12-31"),
    ],
    instruments="csi300",
    backtest_mode="joinquant_like",
    token_mode="postfix",
)

print(formula)
print(summary_df)
```

---

## 5. 实验结论（V0.2 当前状态）

### 5.1 可行解存在但稀疏

在 V0.2 的三段体系 + 牛熊规则下，观测到：
- 存在满足规则的 seed，但通过概率不高；
- 结果对 seed 极其敏感，符合“可行解稀疏 + 高方差搜索”的特征。

通过样本的数值证据、以及失败样本的结构性归因（train 失败 / 熊市 valid 失败 / holdout 失败）已在 `exp/v0.2/exp_log.md` 给出。

### 5.2 对 2025 的定位

2025 的角色是压力测试而非训练目标：
- 如果在 2020-2024 形成稳定可行解集合，再去对 2025 进行归因更符合方法论；
- 归因方向包括：风格漂移、流动性/交易约束变化、信号结构偏移等。

---

## 6. 后续工作方向（V0.2 -> 下一阶段）

V0.2 已完成“目标与评估体系”重构，下一阶段更偏工程化与命中率提升：

1. **训练期 gate 化**：将牛熊规则融入 best 选择机制，而非训练后再筛选。
2. **扩大搜索预算/降低方差**：更多 seed、更长 steps 或更稳健的优化策略。
3. **分层失败诊断**：对失败 seed 按段定位，减少盲目调 reward。
4. **压力测试报告固定化**：将 2025 作为稳定对照项，形成长期可比实验流程。

